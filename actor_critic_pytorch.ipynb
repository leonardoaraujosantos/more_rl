{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic with Pytorch\n",
    "### Introduction\n",
    "This notebook will train a policy using the actor-critic framework, where the policy is trained usign the policy gradient algorithm (Actor) while another network will learn an estimate of the value function (critic). This framework is more sample efficient than just using Policy gradients, it's a way to merge Policy and Value based methods.\n",
    "One big advangage of Actor-Critic models are the sample efficiency. Now we don't need to wait for a full eposide to start learning, we can make an update at each timestep.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "![alt text](imgs/actor_critic.png \"Game\")\n",
    "The main actors are:\n",
    "* Critic: Estimate how good the action taken is. (Input state, output scalar value with estimative)\n",
    "* Actor: Just the policy (Input state, output distribution of actions)\n",
    "\n",
    "#### References\n",
    "* https://medium.com/@henrymao/reinforcement-learning-using-asynchronous-advantage-actor-critic-704147f91686\n",
    "* https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d\n",
    "* https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752\n",
    "* https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2\n",
    "* https://github.com/nikhilbarhate99/Actor-Critic\n",
    "* http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf.pdf\n",
    "* https://www.youtube.com/watch?v=O5BlozCJBSE\n",
    "* https://www.youtube.com/watch?v=bRfUxQs6xIM&t=28s\n",
    "* https://www.youtube.com/watch?v=lvoHnicueoE&t=4s\n",
    "* http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of rewards consider as win: 200\n",
      "Action space: 4\n",
      "Observation space: (8,)\n",
      "Reward range: (-inf, inf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardoaraujo/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/leonardoaraujo/anaconda3/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/leonardoaraujo/anaconda3/lib/python3.8/site-packages/gym/core.py:200: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make('CartPole-v1')\n",
    "# Uncomment for this environment (takes longer to train)\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "num_episodes=3000\n",
    "render = False\n",
    "\n",
    "print('Amount of rewards consider as win:', env.spec.reward_threshold)\n",
    "print('Action space:', env.action_space.n)\n",
    "print('Observation space:', env.observation_space.shape)\n",
    "print('Reward range:', env.reward_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Actor/Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        state_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        num_hidden = 128\n",
    "        \n",
    "        self.affine = nn.Linear(state_space, num_hidden, bias=False)\n",
    "        \n",
    "        # Action and value head\n",
    "        self.action_layer = nn.Linear(num_hidden, action_space, bias=False)\n",
    "        self.value_layer = nn.Linear(num_hidden, 1, bias=False)\n",
    "        \n",
    "        self.logprobs = []\n",
    "        self.state_values = []\n",
    "        self.rewards = []\n",
    "        # Store sum of rewards from each episode\n",
    "        self.reward_history = []\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = F.relu(self.affine(state))\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        # Sample policy action space\n",
    "        action_probs = F.softmax(self.action_layer(state), dim=-1)\n",
    "        action_distribution = Categorical(action_probs)\n",
    "        action = action_distribution.sample()\n",
    "        \n",
    "        self.logprobs.append(action_distribution.log_prob(action))\n",
    "        self.state_values.append(state_value)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def calculateLoss(self, gamma=0.99):\n",
    "        \n",
    "        # calculating discounted rewards:\n",
    "        rewards = []\n",
    "        dis_reward = 0\n",
    "        for reward in self.rewards[::-1]:\n",
    "            dis_reward = reward + gamma * dis_reward\n",
    "            rewards.insert(0, dis_reward)\n",
    "        \n",
    "        # Append total rewards on the episode\n",
    "        self.reward_history.append(np.sum(self.rewards))\n",
    "                \n",
    "        # normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std())\n",
    "        \n",
    "        loss = 0\n",
    "        for logprob, value, reward in zip(self.logprobs, self.state_values, rewards):\n",
    "            advantage = reward  - value.item()\n",
    "            action_loss = -logprob * advantage\n",
    "            #print('value:', torch.squeeze(value, 0))\n",
    "            #print('reward:', reward)\n",
    "            #assert(false)\n",
    "            value_loss = F.smooth_l1_loss(torch.squeeze(value, 0), reward.float())\n",
    "            loss += (action_loss + value_loss)\n",
    "        return loss\n",
    "    \n",
    "    def clearMemory(self):\n",
    "        del self.logprobs[:]\n",
    "        del self.state_values[:]\n",
    "        del self.rewards[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Network and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ActorCritic()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tlength: 100\tmean reward: -1.1930579914161905\n",
      "Episode 50\tlength: 93\tmean reward: -198.21811796085885\n",
      "Episode 100\tlength: 102\tmean reward: -75.82101314361773\n",
      "Episode 150\tlength: 71\tmean reward: -74.0777645937939\n",
      "Episode 200\tlength: 91\tmean reward: -43.24332036505517\n",
      "Episode 250\tlength: 171\tmean reward: -10.806447680769157\n",
      "Episode 300\tlength: 272\tmean reward: -3.9034284002092203\n",
      "Episode 350\tlength: 195\tmean reward: -7.261331415652243\n",
      "Episode 400\tlength: 676\tmean reward: 1.8800774126644595\n",
      "Episode 450\tlength: 282\tmean reward: -33.23401338312611\n",
      "Episode 500\tlength: 196\tmean reward: -69.38725385621258\n",
      "Episode 550\tlength: 999\tmean reward: -1.8933919358282751\n",
      "Episode 600\tlength: 188\tmean reward: 17.70944034711996\n",
      "Episode 650\tlength: 141\tmean reward: 18.001583055754327\n",
      "Episode 700\tlength: 386\tmean reward: 10.109186109827336\n",
      "Episode 750\tlength: 204\tmean reward: 19.565130040236955\n",
      "Episode 800\tlength: 178\tmean reward: 32.11500172577556\n",
      "Episode 850\tlength: 466\tmean reward: 15.692041144045415\n",
      "Episode 900\tlength: 457\tmean reward: 20.741716710552915\n",
      "Episode 950\tlength: 672\tmean reward: 7.17018179134701\n",
      "Episode 1000\tlength: 999\tmean reward: 7.3304622429165365\n",
      "Episode 1050\tlength: 999\tmean reward: 7.028135180402408\n",
      "Episode 1100\tlength: 999\tmean reward: 5.660160577592402\n",
      "Episode 1150\tlength: 999\tmean reward: 5.88245469459044\n",
      "Episode 1200\tlength: 999\tmean reward: 6.366426920801212\n",
      "Episode 1250\tlength: 999\tmean reward: 6.838396572788546\n",
      "Episode 1300\tlength: 999\tmean reward: 7.070837622196688\n",
      "Episode 1350\tlength: 999\tmean reward: 7.199143812698699\n",
      "Episode 1400\tlength: 999\tmean reward: 7.048006075879042\n"
     ]
    }
   ],
   "source": [
    "running_reward = 0\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Run a full eposide\n",
    "    for time in range(1000):\n",
    "        action = policy(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        policy.rewards.append(reward)\n",
    "        running_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Updating the policy :\n",
    "    optimizer.zero_grad()\n",
    "    loss = policy.calculateLoss(gamma)\n",
    "    loss.backward()\n",
    "    optimizer.step()        \n",
    "    policy.clearMemory()\n",
    "\n",
    "    if (running_reward/time) > env.spec.reward_threshold:\n",
    "        print(\"########## Solved! ##########\")\n",
    "        break\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        rewards_per_time = running_reward/time\n",
    "        print('Episode {}\\tlength: {}\\tmean reward: {}'.format(\n",
    "            episode, time, rewards_per_time))\n",
    "        running_reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "# Create grid 2x1 plots\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "\n",
    "# Calculate the mean of the rewards over time\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "# Calculate the standard deviation over time\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "\n",
    "# Plot graph 1\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "# Plot graph 2\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_on_environment(env, model, t_max):\n",
    "    state = env.reset()\n",
    "    for t in range(t_max):\n",
    "        action = policy(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        # Game finished\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "env = gym.wrappers.RecordVideo(gym.make(\"LunarLander-v2\"), 'video')\n",
    "play_on_environment(env, policy, t_max=5000)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
