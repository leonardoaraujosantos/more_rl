{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming (Value Iteration)\n",
    "\n",
    "### Return\n",
    "It's defined as the total discounted reward received from the environment on an infinite horizon\n",
    "$$\\text{Return}=\\sum_{i=0}^{\\infty}\\gamma^i r_i \\therefore [r_0 + \\gamma.r_1 + \\gamma^2.r_2 + \\gamma^3.r_3 + ...\\gamma^\\infty.r_\\infty]$$\n",
    "The reasons for discounted reward are:\n",
    "* Avoid inifintie reward\n",
    "* Model the behaviour of the agent that will prefer imediate rewards ($\\gamma=0$) or long term rewards ($\\gamma=1$)\n",
    "\n",
    "### Value Function\n",
    "It's the return expected value.\n",
    "$$V^\\pi(s)=\\mathbb{E}\\left[\\sum_{i=0}^{\\infty} \\gamma^i r_i\\right] s \\in S$$\n",
    "\n",
    "### Value Iteration\n",
    "This algorithm use the Bellman expectation equation to iteratively find the optimum Value function. The algorithm keep updating the estimative of the value function until it converges. After we have the optimium value function we can act optimally.\n",
    "\n",
    "![alt text](imgs/value_iteration_orig.png \"Game\")\n",
    "\n",
    "$$Q_\\text{k+1}(s,a)=\\sum_{\\text{s'}}P(s'|s,a).(R(s,a,s') + \\gamma V_k(s')) \\text{     } \\forall k \\geq 0$$\n",
    "$$V_k(s)=max_a(Q_k(s,a)) \\text{     } \\forall k > 0$$\n",
    "\n",
    "$$V^*(s)=max_a[R(s,a)+\\gamma . \\sum_{\\text{s'}}P(s'|s,a).V^*(s')]$$\n",
    "\n",
    "You can either save the $V(s)$ array or the $Q(s,a)$ array. Saving the V array results in less storage, but it is more difficult to determine an optimal action, and one more iteration is needed to determine which action results in the greatest value. Storing the Q array you have imediatelly the action with biggest value to each state.\n",
    "\n",
    "### References\n",
    "* https://medium.com/@m.alzantot/deep-reinforcement-learning-demystified-episode-0-2198c05a6124\n",
    "* https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa\n",
    "* https://artint.info/html/ArtInt_227.html\n",
    "* https://pt.overleaf.com/learn/latex/Integrals,_sums_and_limits\n",
    "* https://www.youtube.com/watch?v=c5X7Xj1iYak\n",
    "* https://en.wikipedia.org/wiki/Bellman_equation\n",
    "* https://www.youtube.com/watch?v=KovN7WKI9Y0\n",
    "* https://www.youtube.com/watch?v=glHKJ359Cnc\n",
    "* https://www.youtube.com/watch?v=cTu7mvRE354"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyper parameters\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardo_a/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "#env_name  = 'FrozenLake8x8-v0'\n",
    "#env_name = 'FrozenLake-v0'\n",
    "env_name = 'Taxi-v2'\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Environment Model\n",
    "This part of the code will expose a dictionary $P(state,action)$ that will return a list of tupples $(prob,\\text{next_state},reward,done)$, this will be the list of possible next states from that particular (state,action) pair, while P exposes the MDP of the environment.\n",
    "\n",
    "For example this code would return the same thing\n",
    "```python\n",
    "state_reset = env.reset()\n",
    "print('reset state:',state_reset)\n",
    "action=0\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print('############')\n",
    "print('Next state:',next_state)\n",
    "print('Reward:',reward)\n",
    "print('done:',done)\n",
    "print('info:',info)\n",
    "\n",
    "#reset state: 0\n",
    "############\n",
    "#Next state: 0\n",
    "#Reward: 0.0\n",
    "#done: False\n",
    "#info: {'prob': 0.3333333333333333}\n",
    "\n",
    "# Return all the possible states with the tupple (probability, next_state, reward, done)\n",
    "env.P[state_reset][action]\n",
    "# [(0.3333333333333333, 0, 0.0, False),\n",
    "# (0.3333333333333333, 0, 0.0, False),\n",
    "# (0.3333333333333333, 8, 0.0, False)]\n",
    "```\n",
    "\n",
    "Just few environments expose their MDP:\n",
    "* FrozenLake-v0\n",
    "* FrozenLake8x8-v0\n",
    "* Taxi-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have access to the MDP of the environment\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display search space size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space: 3000\n"
     ]
    }
   ],
   "source": [
    "num_states = None\n",
    "num_actions = None\n",
    "try:\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    print('Search space:',num_states*num_actions)\n",
    "except:\n",
    "    num_states = np.prod(env.observation_space.shape)\n",
    "    num_actions = np.prod(env.action_space.shape)\n",
    "    print('Search space:',num_states*num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a policy from optimum Value Function\n",
    "As mentioned earlier if we have the optimum value function we can get the optimum policy. If you have the action-value function the job is actually easier but if you have the value function you need to do the following:\n",
    "* Select a particular state\n",
    "* for each action from that particular state\n",
    "* Query from the transition function all the possible states from that particular state,action\n",
    "* Choose the action with biggest value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for each state s do \n",
    "    \n",
    "    π[s] = argmaxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk[s']) \n",
    " \n",
    " return π,Vk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy_from_value(v, gamma = 1.0):\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    # For each state\n",
    "    for s in range(env.observation_space.n):\n",
    "        #for a in range(env.action_space.n):\n",
    "            #policy[s] = np.argmax(sum([p*(r + gamma*v[s_]) for p,s_,r,_ in env.P[s][a]]))\n",
    "        policy[s] = np.argmax([sum([p*(r + gamma*v[s_]) for p,s_,r,_ in env.P[s][a]]) for a in range(env.action_space.n)])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration Algorithm\n",
    "\n",
    "#### Bellman Optimality Equation\n",
    "$$V^*(s)=max_a(Q^{\\pi^*}(s,a))$$\n",
    "\n",
    "The value iteration algorithm has the following structure:\n",
    "\n",
    "1. Start with a zeroed value function\n",
    "2. Use the bellman optimality equation to get an estimative of the optimum value function.\n",
    "3. Repeat step 2 until the value function doesnt change anymore.\n",
    "![alt text](imgs/value_iteration.png \"Game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma = 1.0, max_iterations = 100000):\n",
    "    # Initialize Value function with zeros\n",
    "    v = np.zeros(env.observation_space.n)  # initialize value-function\n",
    "    q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    # Big number of iterations\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        # For each state\n",
    "        for s in range(env.observation_space.n):\n",
    "            for a in range(env.action_space.n):\n",
    "                q[s,a] = sum([p*(r + gamma*prev_v[s_]) for p, s_, r, _ in env.P[s][a]])\n",
    "            # Convert action-value(Q) function into value function(V)\n",
    "            v[s] = max(q[s,:])\n",
    "        \n",
    "        # Check convergence (Basically check if the value function didn't change much)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= np.finfo(float).eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Value Iteration Algorithm and get the best policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 3270.\n",
      "CPU times: user 13.6 s, sys: 28.2 ms, total: 13.6 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the value iteration\n",
    "optimal_v = value_iteration(env, gamma);\n",
    "policy = extract_policy_from_value(optimal_v, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 16\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "while not done:\n",
    "    action = int(policy[state])\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    state = next_state\n",
    "    step += 1\n",
    "    print(\"Step: {}\".format(step))\n",
    "    env.render()\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
