{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients (Reinforce)\n",
    "\n",
    "The REINFORCE algorithm is one implementation of the Policy Gradient family of algorithms, the idea of the algorithm is to have the policy represented by a neural network with $\\theta$ parameters. The main idea of the algorithm is to change the network parameters to make the actions that provided positive rewards to be more probable to happen. The training phase optimize the following loss function:\n",
    "$$\\mathcal{L}=-\\hat{Q}(s,a).log\\pi_{\\theta}(a|s)$$\n",
    "Where:\n",
    "* $\\hat{Q}(s,a)$: It's an estimate of the state action function, that will modulate the probability of good actions to happen more often. This is also called **Advantage**.\n",
    "* $\\pi_{\\theta}(a|s)$: It's the neural network that represent the policy and returns an distribution of actions given an state.\n",
    "\n",
    "The steps of the algorithm\n",
    "1. Initialize Network at random\n",
    "2. Play N episodes saving their transitions \"k\" (s,a,r,s')\n",
    "3. For every episode calculate $\\hat{Q}(s,a)=\\sum_{i=0} \\gamma^i r_i$\n",
    "4. Perform SGD to minimize the loss: $\\mathcal{L}=-\\hat{Q}(s,a).log\\pi_{\\theta}(a|s)$\n",
    "5. Repeat 2 until convergence \n",
    "\n",
    "#### Disadvantages of Policy Gradients\n",
    "* Full episodes are required, we need to wait a full episode to complete\n",
    "* High Gradients Variance: This issue can be handled by subtracting a baseline from the Value estimation\n",
    "* Exploration: The agent can converge to a local-optimal area and won't explore efficiently anymore. This can be solved by the Entropy Bonus technique that basically subtract the entropy of the policy from the loss function.\n",
    "* Correlation between samples: This can be remedy by using parallel environments with same policy and using the experiences from different environments to train the policy.\n",
    "* Less sample efficient: To deal with this we need another algorithm (Actor-Critic)\n",
    "\n",
    "The image bellow can highlight the full episodic issue, where we have 2 trajectories where one or more actions could be bad, but as the final total score is good those bad actions will be averaged. That's one of the reasons why Policy Gradient methods are less sample efficient.\n",
    "\n",
    "![alt text](imgs/episode_problem.png \"Game\")\n",
    "\n",
    "Consider that the Advantage will \"label\" each experience on the epsisode trajectory, saying if the action was good or bad.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Install\n",
    "```bash\n",
    "pip install gym\n",
    "pip install box2d\n",
    "pip install ffmpeg\n",
    "pip install imageio-ffmpeg\n",
    "pip install pygame\n",
    "```\n",
    "\n",
    "### References\n",
    "* [Reference Implementation](https://towardsdatascience.com/breaking-down-richard-suttons-policy-gradient-9768602cb63b)\n",
    "* [Medium Article](https://towardsdatascience.com/policy-gradient-methods-104c783251e0) and [Source Code](https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_19_REINFORCE_Algorithm.ipynb)\n",
    "* [Lunar Lander - Deep Reinforcement Learning, Noise Robustness, and Quantization](https://xusophia.github.io/DataSciFinalProj/)\n",
    "* [Policy Gradient Demystified](https://amoudgl.github.io/blog/blog/policy-gradient/)\n",
    "* [Policy Gradient CartPole](https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_19_REINFORCE_Algorithm.ipynb) and [Article](https://towardsdatascience.com/policy-gradient-methods-104c783251e0)\n",
    "* [PPO From Scratch](https://medium.com/analytics-vidhya/coding-ppo-from-scratch-with-pytorch-part-1-4-613dfc1b14c8)\n",
    "* [RL Book Sample](https://github.com/PacktPublishing/Hands-on-Reinforcement-Learning-with-PyTorch/blob/master/Section%204/4.3%20Policy%20Gradients%20REINFORCE%20Baseline.ipynb)\n",
    "* [Karpathy lecture on OpenAI Summercamp](https://www.youtube.com/watch?v=tqrcjHuNdmQ)\n",
    "* [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)\n",
    "* [Reinforce with Cartpole environment](https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf)\n",
    "* [On Policy vs Off Policy](https://leimao.github.io/blog/RL-On-Policy-VS-Off-Policy/)\n",
    "* [An Intuitive Explanation of Policy Gradient](https://towardsdatascience.com/an-intuitive-explanation-of-policy-gradient-part-1-reinforce-aa4392cbfd3c)\n",
    "* https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f\n",
    "* https://gist.github.com/tamlyn/a9d2b3990f9dab0f82d1dfc1588c876a\n",
    "* http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n",
    "* [UCL RL Course](https://www.youtube.com/watch?v=iOh7QUZGyiU&list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs)\n",
    "* [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)\n",
    "* https://medium.com/@gabogarza/deep-reinforcement-learning-policy-gradients-8f6df70404e6\n",
    "* https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4\n",
    "* http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "num_episodes = 100000\n",
    "t_max=5000\n",
    "# Training hyperparameters\n",
    "\n",
    "hidden_size = 64\n",
    "learning_rate = 0.0001\n",
    "gamma = 0.999\n",
    "\n",
    "def save_best_model(model, file='pg_best.pth.tar'):\n",
    "    # save the model\n",
    "    torch.save(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 8\n",
      "Number of actions: 4\n",
      "Amount of rewards consider as win: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardoaraujo/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/leonardoaraujo/anaconda3/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "win_threshold = env.spec.reward_threshold\n",
    "print('Number of states:', n_states)\n",
    "print('Number of actions:', n_actions)\n",
    "print('Amount of rewards consider as win:', env.spec.reward_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Approximation\n",
    "This network will learn a policy, the input will be a state, and the output the logits that will represent the actions. \n",
    "$$\\Pi(a|s)_\\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        self.softmax_activation = torch.nn.Softmax(dim=0)\n",
    "        self.action_space = [action for action in range(n_actions)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        action_probabilities = self.softmax_activation(x)\n",
    "        action_idx = self.get_action(action_probabilities)\n",
    "        log_prob_action = torch.log(action_probabilities.squeeze(0))[action_idx]\n",
    "        return action_idx, log_prob_action\n",
    "    \n",
    "    def get_action(self, action_probs):\n",
    "        # Choose a random action(integer) modulated by the policy action probabilities\n",
    "        #return np.random.choice(self.action_space, p=action_probs.squeeze(0).detach().cpu().numpy())\n",
    "        action_distribution = Categorical(action_probs)\n",
    "        return action_distribution.sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Neural Network/Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network (Function approximation)\n",
    "policy_net = Net(n_states, hidden_size, n_actions)\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "After several thousand episodes, something cool start to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\treward_mean: -1.14 \tloss:0.53 \ttotal_reward:-77.20529055939085\n",
      "Save best reward mean: 0.040139707006367376\n",
      "Save best reward mean: 0.3316600251334706\n",
      "Save best reward mean: 0.534146797281234\n",
      "episode 500\treward_mean: -0.88 \tloss:-1.20 \ttotal_reward:-59.67683586500082\n",
      "episode 1000\treward_mean: -2.67 \tloss:-1.40 \ttotal_reward:-299.0232360720189\n",
      "episode 1500\treward_mean: -0.73 \tloss:1.98 \ttotal_reward:-80.00508452940687\n",
      "episode 2000\treward_mean: -0.15 \tloss:-2.87 \ttotal_reward:-20.290480749444143\n",
      "episode 2500\treward_mean: -0.98 \tloss:0.16 \ttotal_reward:-82.25308165612446\n",
      "episode 3000\treward_mean: 0.04 \tloss:1.53 \ttotal_reward:3.758901449964199\n",
      "episode 3500\treward_mean: -1.04 \tloss:-4.98 \ttotal_reward:-77.02299161083172\n",
      "episode 4000\treward_mean: -0.25 \tloss:-0.27 \ttotal_reward:-46.13469049646839\n",
      "episode 4500\treward_mean: -0.89 \tloss:-1.98 \ttotal_reward:-69.98414367402023\n",
      "episode 5000\treward_mean: -0.93 \tloss:-0.87 \ttotal_reward:-73.53510551567436\n",
      "episode 5500\treward_mean: -0.58 \tloss:2.66 \ttotal_reward:-49.42259104555653\n",
      "episode 6000\treward_mean: -0.44 \tloss:4.65 \ttotal_reward:-63.39046846239421\n"
     ]
    }
   ],
   "source": [
    "best_mean_reward = 0\n",
    "list_sum_rewards_episode = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment\n",
    "    curr_state = env.reset()\n",
    "    prev_state = curr_state\n",
    "    done = False\n",
    "    trajectory = [] \n",
    "    list_log_action_prob = []\n",
    "    total_reward = 0\n",
    "    rewards_on_episode = []\n",
    "    discounted_rewards = []\n",
    "    \n",
    "    # Play an episode to the END\n",
    "    for t in range(t_max):\n",
    "        action, log_action_prob = policy_net(torch.from_numpy(curr_state).float())\n",
    "        curr_state, reward, done, info = env.step(action) \n",
    "        total_reward += reward\n",
    "        rewards_on_episode.append(reward)\n",
    "        list_log_action_prob.append(log_action_prob)\n",
    "        # Save the trajectory on episode.\n",
    "        trajectory.append((curr_state, action, reward, prev_state)) \n",
    "        prev_state = curr_state\n",
    "        if done: \n",
    "            break\n",
    "            \n",
    "    # Select the rewards\n",
    "    reward_batch = torch.Tensor([r for (s,a,r,s_prev) in trajectory])\n",
    "    \n",
    "    # Calculate the discounted rewards\n",
    "    discounted_rewards = np.zeros_like(reward_batch)\n",
    "    for t in range(len(reward_batch)):\n",
    "        G_sum = 0\n",
    "        discount = 1\n",
    "        for k in range(t, len(reward_batch)):\n",
    "            G_sum += reward_batch[k] * discount\n",
    "            discount *= gamma\n",
    "        discounted_rewards[t] = G_sum\n",
    "    discounted_rewards = torch.Tensor(discounted_rewards)\n",
    "\n",
    "    \n",
    "    # Calculate Advantage\n",
    "    advantage = (discounted_rewards - torch.mean(discounted_rewards)) / (torch.std(discounted_rewards))\n",
    "    \n",
    "    # Convert list of log probabilties of actions into a pytorch tensor\n",
    "    log_prob_actions = torch.stack(list_log_action_prob)\n",
    "    \n",
    "    # Calculate Loss \n",
    "    # (observe that the advantage will modulate each action probability with it's discounted rewards)\n",
    "    loss = (-log_prob_actions*advantage).sum()\n",
    "    \n",
    "    # Update policy weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    list_sum_rewards_episode.append(total_reward)\n",
    "    \n",
    "    mean_reward = np.mean(rewards_on_episode)\n",
    "    if (episode % 500) == 0:\n",
    "        print('episode {}\\treward_mean: {:.2f} \\tloss:{:.2f} \\ttotal_reward:{}'.format(\n",
    "            episode, mean_reward, loss.item(), total_reward))\n",
    "    \n",
    "    if mean_reward > best_mean_reward:\n",
    "        print('Save best reward mean:', mean_reward)\n",
    "        save_best_model(policy_net)\n",
    "        best_mean_reward = mean_reward\n",
    "    \n",
    "    # If the mean reward is bigger than the completion score threshold we stop\n",
    "    if mean_reward > win_threshold:\n",
    "        print(\"Environment has been successfullly completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "# Create grid 2x1 plots\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "\n",
    "# Calculate the mean of the rewards over time\n",
    "rolling_mean = pd.Series(list_sum_rewards_episode).rolling(window).mean()\n",
    "# Calculate the standard deviation over time\n",
    "std = pd.Series(list_sum_rewards_episode).rolling(window).std()\n",
    "\n",
    "# Plot graph 1\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(list_sum_rewards_episode)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "# Plot graph 2\n",
    "ax2.plot(list_sum_rewards_episode)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out the best model\n",
    "You can locally run on the enviornment but this command will generate a video, which is better to run from Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy_net = torch.load('pg_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_on_environment(env, model, t_max):\n",
    "    state = env.reset()\n",
    "    for t in range(t_max):\n",
    "        action, _ = model(torch.from_numpy(state).float())\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        # Game finished\n",
    "        if done:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordVideo(gym.make(\"LunarLander-v2\"), 'video')\n",
    "play_on_environment(env, policy_net, t_max=5000)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
