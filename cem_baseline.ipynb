{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Cross Entropy Method\n",
    "\n",
    "### Things to install\n",
    "```bash\n",
    "pip install gym\n",
    "pip install box2d\n",
    "pip install ffmpeg\n",
    "pip install imageio-ffmpeg\n",
    "pip install pygame\n",
    "```\n",
    "\n",
    "### References\n",
    "* [Medium Article](https://medium.com/coinmonks/landing-a-rocket-with-simple-reinforcement-learning-3a0265f8b58c) and [Source Code](https://github.com/djbyrne/Landing-A-Rocket-With-Simple-Reinforcement-Learning/blob/master/Landing%20A%20Rocket%20With%20Simple%20Reinforcement%20Learning.ipynb)\n",
    "* [Other Article](https://towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737)\n",
    "* [Link between Cross Entropy and Policy Gradient](https://medium.com/intro-to-artificial-intelligence/a-link-between-cross-entropy-and-policy-gradient-expression-b2b308511867)\n",
    "* [RL tutorial with Mujoco](https://medium.com/swlh/getting-started-with-reinforcement-learning-mujoco-and-openai-gym-67243b78b599)\n",
    "* [Mujoco Tutorial](https://www.youtube.com/watch?v=j1nCeqtfySQ) and [Installing on Ubuntu](https://www.youtube.com/watch?v=Wnb_fiStFb8)\n",
    "* [Mujoco and Unity](https://www.youtube.com/watch?v=eyzzsGJ1iic)\n",
    "* [Rocket lander](https://github.com/EmbersArc/gym-rocketlander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.wrappers\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "session_size = 500\n",
    "\n",
    "# Filter for top 20 experiences\n",
    "percentile = 80\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 100\n",
    "hidden_size = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Each environment will have a different completion score\n",
    "completion_score = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardoaraujo/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/leonardoaraujo/anaconda3/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "print('Number of states:', n_states)\n",
    "print('Number of actions:', n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Function Approximation\n",
    "This network will learn a policy, the input will be a state, and the output the logits that will represent the actions. \n",
    "$$\\Pi(a|s)_\\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data and Filter Batches\n",
    "The idea here is to make the agent play some games, then return a batch of experiences. Later those experiences will be filter out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_experience_on_environment(env, policy_net, batch_size, t_max=5000):\n",
    "    \n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states,actions = [],[]\n",
    "        total_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(t_max):\n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probs_v = activation(policy_net(s_v))\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            # Explore/Exploit a bit \n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "            # Act on the environment\n",
    "            new_s, r, done, info = env.step(a)\n",
    "            \n",
    "            # Save experience (State,Action,Reward)\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "            s = new_s\n",
    "            \n",
    "            # Game finished\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "                \n",
    "    return batch_states, batch_actions, batch_rewards\n",
    "\n",
    "\n",
    "def filter_batch(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "    for i in range(len(rewards_batch)):\n",
    "        # Filter experience with \"good\" reward (ie: top 20 or 80% percentile)\n",
    "        if rewards_batch[i] > reward_threshold:\n",
    "            for j in range(len(states_batch[i])):\n",
    "                elite_states.append(states_batch[i][j])\n",
    "                elite_actions.append(actions_batch[i][j])\n",
    "    \n",
    "    return elite_states,elite_actions\n",
    "\n",
    "def save_best_model(model, file='cem_policy_best.pth.tar'):\n",
    "    # save the model\n",
    "    torch.save(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Neural Network/Optimiser and Cross Entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network (Function approximation)\n",
    "policy_net = Net(n_states, hidden_size, n_actions)\n",
    "\n",
    "# Cross Entropy Loss\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go learn Mr. robot!\n",
    "It will take some time but eventually the policy will be learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.386, reward_mean=-168.8, reward_threshold=-97.2\n",
      "1: loss=1.352, reward_mean=-203.7, reward_threshold=-113.1\n",
      "2: loss=1.316, reward_mean=-192.7, reward_threshold=-88.7\n",
      "3: loss=1.267, reward_mean=-232.0, reward_threshold=-85.8\n",
      "4: loss=1.233, reward_mean=-238.9, reward_threshold=-114.1\n",
      "5: loss=1.211, reward_mean=-189.0, reward_threshold=-74.9\n",
      "6: loss=1.224, reward_mean=-161.5, reward_threshold=-68.3\n",
      "7: loss=1.216, reward_mean=-130.8, reward_threshold=-55.6\n",
      "8: loss=1.231, reward_mean=-135.2, reward_threshold=-67.0\n",
      "9: loss=1.231, reward_mean=-100.5, reward_threshold=-42.6\n",
      "10: loss=1.226, reward_mean=-94.4, reward_threshold=-57.3\n",
      "11: loss=1.197, reward_mean=-77.3, reward_threshold=-27.4\n",
      "12: loss=1.199, reward_mean=-83.4, reward_threshold=-30.0\n",
      "13: loss=1.164, reward_mean=-87.2, reward_threshold=-28.6\n",
      "14: loss=1.125, reward_mean=-101.1, reward_threshold=-19.1\n",
      "15: loss=1.114, reward_mean=-116.6, reward_threshold=-15.0\n",
      "16: loss=1.108, reward_mean=-168.8, reward_threshold=-43.7\n",
      "17: loss=1.064, reward_mean=-194.0, reward_threshold=-66.4\n",
      "18: loss=1.117, reward_mean=-171.3, reward_threshold=-36.8\n",
      "19: loss=1.098, reward_mean=-132.4, reward_threshold=-21.5\n",
      "20: loss=1.124, reward_mean=-80.0, reward_threshold=-4.4\n",
      "21: loss=1.142, reward_mean=-40.4, reward_threshold=2.6\n",
      "22: loss=1.136, reward_mean=-26.5, reward_threshold=14.9\n",
      "23: loss=1.140, reward_mean=-13.9, reward_threshold=15.0\n",
      "24: loss=1.133, reward_mean=-18.6, reward_threshold=11.2\n",
      "25: loss=1.162, reward_mean=-15.7, reward_threshold=11.8\n",
      "26: loss=1.150, reward_mean=-3.4, reward_threshold=25.0\n",
      "27: loss=1.142, reward_mean=-7.3, reward_threshold=23.0\n",
      "28: loss=1.156, reward_mean=-16.7, reward_threshold=32.0\n",
      "29: loss=1.121, reward_mean=-27.5, reward_threshold=29.5\n",
      "30: loss=1.070, reward_mean=-34.8, reward_threshold=29.2\n",
      "31: loss=1.049, reward_mean=-66.6, reward_threshold=21.9\n",
      "32: loss=1.063, reward_mean=-47.2, reward_threshold=26.3\n",
      "33: loss=1.101, reward_mean=-25.6, reward_threshold=26.4\n",
      "34: loss=1.081, reward_mean=-18.9, reward_threshold=37.9\n",
      "35: loss=1.028, reward_mean=6.1, reward_threshold=36.8\n",
      "Save best reward: 6.081570153930524\n",
      "36: loss=1.024, reward_mean=12.1, reward_threshold=32.4\n",
      "Save best reward: 12.065777107316471\n",
      "37: loss=0.987, reward_mean=11.5, reward_threshold=39.8\n",
      "38: loss=0.922, reward_mean=14.4, reward_threshold=40.6\n",
      "Save best reward: 14.408650368003567\n",
      "39: loss=0.911, reward_mean=17.2, reward_threshold=38.7\n",
      "Save best reward: 17.210920649722684\n",
      "40: loss=0.956, reward_mean=12.1, reward_threshold=35.7\n",
      "41: loss=0.961, reward_mean=20.4, reward_threshold=41.1\n",
      "Save best reward: 20.411209719234986\n",
      "42: loss=0.953, reward_mean=20.4, reward_threshold=50.3\n",
      "43: loss=0.895, reward_mean=15.9, reward_threshold=41.0\n",
      "44: loss=0.966, reward_mean=19.7, reward_threshold=52.7\n",
      "45: loss=0.964, reward_mean=22.2, reward_threshold=53.6\n",
      "Save best reward: 22.21535207517866\n",
      "46: loss=0.914, reward_mean=22.9, reward_threshold=60.5\n",
      "Save best reward: 22.884074836327372\n",
      "47: loss=0.923, reward_mean=12.3, reward_threshold=47.5\n",
      "48: loss=0.847, reward_mean=17.9, reward_threshold=73.9\n",
      "49: loss=0.824, reward_mean=4.7, reward_threshold=71.7\n",
      "50: loss=0.777, reward_mean=24.7, reward_threshold=140.9\n",
      "Save best reward: 24.69660108475071\n",
      "51: loss=0.739, reward_mean=50.7, reward_threshold=179.5\n",
      "Save best reward: 50.736091003867344\n",
      "52: loss=0.707, reward_mean=43.6, reward_threshold=204.8\n",
      "53: loss=0.703, reward_mean=56.8, reward_threshold=197.6\n",
      "Save best reward: 56.8353775677095\n",
      "54: loss=0.692, reward_mean=58.2, reward_threshold=200.6\n",
      "Save best reward: 58.18160455076901\n",
      "55: loss=0.681, reward_mean=56.9, reward_threshold=195.3\n",
      "56: loss=0.663, reward_mean=73.5, reward_threshold=212.6\n",
      "Save best reward: 73.5016274813229\n",
      "57: loss=0.647, reward_mean=99.9, reward_threshold=223.5\n",
      "Save best reward: 99.93734256692805\n",
      "58: loss=0.707, reward_mean=99.8, reward_threshold=224.9\n",
      "59: loss=0.686, reward_mean=89.3, reward_threshold=225.6\n",
      "60: loss=0.684, reward_mean=98.1, reward_threshold=224.4\n",
      "61: loss=0.667, reward_mean=92.9, reward_threshold=228.7\n",
      "62: loss=0.698, reward_mean=87.2, reward_threshold=212.3\n",
      "63: loss=0.659, reward_mean=103.0, reward_threshold=229.5\n",
      "Save best reward: 103.01548613733921\n",
      "64: loss=0.680, reward_mean=95.7, reward_threshold=223.0\n",
      "65: loss=0.676, reward_mean=98.9, reward_threshold=204.4\n",
      "66: loss=0.659, reward_mean=85.5, reward_threshold=216.5\n",
      "67: loss=0.684, reward_mean=105.1, reward_threshold=229.8\n",
      "Save best reward: 105.12270274611137\n",
      "68: loss=0.693, reward_mean=115.9, reward_threshold=232.7\n",
      "Save best reward: 115.87318193292036\n",
      "69: loss=0.683, reward_mean=77.0, reward_threshold=213.5\n",
      "70: loss=0.704, reward_mean=100.4, reward_threshold=215.8\n",
      "71: loss=0.658, reward_mean=53.9, reward_threshold=214.1\n",
      "72: loss=0.663, reward_mean=93.2, reward_threshold=220.5\n",
      "73: loss=0.623, reward_mean=85.6, reward_threshold=231.6\n",
      "74: loss=0.605, reward_mean=65.8, reward_threshold=208.3\n",
      "75: loss=0.593, reward_mean=67.1, reward_threshold=225.2\n",
      "76: loss=0.586, reward_mean=92.4, reward_threshold=215.8\n",
      "77: loss=0.598, reward_mean=101.2, reward_threshold=230.9\n",
      "78: loss=0.560, reward_mean=86.8, reward_threshold=224.0\n",
      "79: loss=0.601, reward_mean=100.3, reward_threshold=234.1\n",
      "80: loss=0.584, reward_mean=94.4, reward_threshold=226.7\n",
      "81: loss=0.624, reward_mean=85.9, reward_threshold=217.9\n",
      "82: loss=0.636, reward_mean=100.7, reward_threshold=239.4\n",
      "83: loss=0.613, reward_mean=101.1, reward_threshold=233.8\n",
      "84: loss=0.611, reward_mean=108.2, reward_threshold=228.0\n",
      "85: loss=0.615, reward_mean=110.4, reward_threshold=229.6\n",
      "86: loss=0.565, reward_mean=97.0, reward_threshold=221.4\n",
      "87: loss=0.546, reward_mean=81.3, reward_threshold=227.6\n",
      "88: loss=0.565, reward_mean=86.0, reward_threshold=228.8\n",
      "89: loss=0.517, reward_mean=87.3, reward_threshold=221.9\n",
      "90: loss=0.481, reward_mean=67.3, reward_threshold=204.2\n",
      "91: loss=0.518, reward_mean=84.1, reward_threshold=233.4\n",
      "92: loss=0.548, reward_mean=65.3, reward_threshold=211.1\n",
      "93: loss=0.615, reward_mean=115.6, reward_threshold=239.4\n",
      "94: loss=0.618, reward_mean=85.4, reward_threshold=235.3\n",
      "95: loss=0.649, reward_mean=81.0, reward_threshold=215.5\n",
      "96: loss=0.662, reward_mean=110.7, reward_threshold=228.6\n",
      "97: loss=0.675, reward_mean=98.0, reward_threshold=211.7\n",
      "98: loss=0.711, reward_mean=114.4, reward_threshold=215.8\n",
      "99: loss=0.797, reward_mean=69.7, reward_threshold=201.9\n",
      "100: loss=0.801, reward_mean=80.3, reward_threshold=186.9\n",
      "101: loss=0.805, reward_mean=71.0, reward_threshold=185.7\n",
      "102: loss=0.787, reward_mean=80.5, reward_threshold=195.9\n",
      "103: loss=0.811, reward_mean=89.5, reward_threshold=189.3\n",
      "104: loss=0.821, reward_mean=82.2, reward_threshold=199.8\n",
      "105: loss=0.740, reward_mean=94.3, reward_threshold=204.5\n",
      "106: loss=0.769, reward_mean=82.5, reward_threshold=203.7\n",
      "107: loss=0.729, reward_mean=93.7, reward_threshold=213.7\n",
      "108: loss=0.711, reward_mean=97.6, reward_threshold=221.1\n",
      "109: loss=0.662, reward_mean=95.0, reward_threshold=222.7\n",
      "110: loss=0.705, reward_mean=101.9, reward_threshold=220.7\n",
      "111: loss=0.615, reward_mean=64.9, reward_threshold=209.5\n",
      "112: loss=0.635, reward_mean=124.6, reward_threshold=237.4\n",
      "Save best reward: 124.64336019779888\n",
      "113: loss=0.662, reward_mean=123.1, reward_threshold=243.2\n",
      "114: loss=0.625, reward_mean=120.1, reward_threshold=234.1\n",
      "115: loss=0.608, reward_mean=123.0, reward_threshold=240.9\n",
      "116: loss=0.646, reward_mean=128.6, reward_threshold=239.5\n",
      "Save best reward: 128.6264465164411\n",
      "117: loss=0.646, reward_mean=149.7, reward_threshold=249.2\n",
      "Save best reward: 149.65259276400704\n",
      "118: loss=0.628, reward_mean=139.1, reward_threshold=234.4\n",
      "119: loss=0.616, reward_mean=160.3, reward_threshold=244.5\n",
      "Save best reward: 160.26392650355518\n",
      "120: loss=0.634, reward_mean=132.8, reward_threshold=243.2\n",
      "121: loss=0.641, reward_mean=120.7, reward_threshold=241.8\n",
      "122: loss=0.608, reward_mean=123.9, reward_threshold=249.5\n",
      "123: loss=0.622, reward_mean=135.3, reward_threshold=248.5\n",
      "124: loss=0.655, reward_mean=122.7, reward_threshold=245.4\n",
      "125: loss=0.619, reward_mean=162.9, reward_threshold=254.1\n",
      "Save best reward: 162.94418225711593\n",
      "126: loss=0.652, reward_mean=134.4, reward_threshold=238.9\n",
      "127: loss=0.701, reward_mean=153.3, reward_threshold=247.2\n",
      "128: loss=0.696, reward_mean=173.3, reward_threshold=252.6\n",
      "Save best reward: 173.32741672772602\n",
      "129: loss=0.715, reward_mean=153.6, reward_threshold=242.2\n",
      "130: loss=0.719, reward_mean=120.9, reward_threshold=225.1\n",
      "131: loss=0.826, reward_mean=86.8, reward_threshold=129.2\n",
      "132: loss=0.780, reward_mean=95.9, reward_threshold=187.2\n",
      "133: loss=0.742, reward_mean=113.5, reward_threshold=223.0\n",
      "134: loss=0.767, reward_mean=141.6, reward_threshold=235.1\n",
      "135: loss=0.719, reward_mean=147.1, reward_threshold=236.8\n",
      "136: loss=0.703, reward_mean=136.6, reward_threshold=233.2\n",
      "137: loss=0.711, reward_mean=134.4, reward_threshold=235.5\n",
      "138: loss=0.686, reward_mean=157.9, reward_threshold=236.1\n",
      "139: loss=0.688, reward_mean=125.3, reward_threshold=239.7\n",
      "140: loss=0.723, reward_mean=138.3, reward_threshold=237.5\n",
      "141: loss=0.704, reward_mean=141.0, reward_threshold=239.8\n",
      "142: loss=0.679, reward_mean=123.9, reward_threshold=236.3\n",
      "143: loss=0.648, reward_mean=133.3, reward_threshold=239.8\n",
      "144: loss=0.672, reward_mean=117.4, reward_threshold=240.0\n",
      "145: loss=0.592, reward_mean=141.1, reward_threshold=243.9\n",
      "146: loss=0.637, reward_mean=119.7, reward_threshold=234.2\n",
      "147: loss=0.631, reward_mean=112.3, reward_threshold=240.1\n",
      "148: loss=0.643, reward_mean=114.9, reward_threshold=243.5\n",
      "149: loss=0.673, reward_mean=135.1, reward_threshold=240.3\n",
      "150: loss=0.583, reward_mean=124.0, reward_threshold=249.8\n",
      "151: loss=0.614, reward_mean=105.7, reward_threshold=235.0\n",
      "152: loss=0.678, reward_mean=141.2, reward_threshold=248.9\n",
      "153: loss=0.687, reward_mean=144.8, reward_threshold=248.0\n",
      "154: loss=0.717, reward_mean=149.9, reward_threshold=246.6\n",
      "155: loss=0.723, reward_mean=143.1, reward_threshold=249.0\n",
      "156: loss=0.662, reward_mean=157.8, reward_threshold=252.0\n",
      "157: loss=0.700, reward_mean=156.3, reward_threshold=244.6\n",
      "158: loss=0.626, reward_mean=136.3, reward_threshold=239.4\n",
      "159: loss=0.714, reward_mean=158.2, reward_threshold=240.2\n",
      "160: loss=0.707, reward_mean=137.3, reward_threshold=233.8\n",
      "161: loss=0.713, reward_mean=158.9, reward_threshold=239.7\n",
      "162: loss=0.694, reward_mean=130.9, reward_threshold=226.8\n",
      "163: loss=0.685, reward_mean=143.9, reward_threshold=233.0\n",
      "164: loss=0.727, reward_mean=142.5, reward_threshold=235.4\n",
      "165: loss=0.685, reward_mean=131.9, reward_threshold=232.7\n",
      "166: loss=0.719, reward_mean=148.9, reward_threshold=235.4\n",
      "167: loss=0.721, reward_mean=116.4, reward_threshold=231.8\n",
      "168: loss=0.734, reward_mean=147.4, reward_threshold=226.4\n",
      "169: loss=0.763, reward_mean=148.4, reward_threshold=224.7\n",
      "170: loss=0.746, reward_mean=144.2, reward_threshold=226.6\n",
      "171: loss=0.766, reward_mean=145.9, reward_threshold=228.9\n",
      "172: loss=0.728, reward_mean=119.1, reward_threshold=218.1\n",
      "173: loss=0.742, reward_mean=115.4, reward_threshold=223.0\n",
      "174: loss=0.698, reward_mean=138.1, reward_threshold=229.6\n",
      "175: loss=0.701, reward_mean=118.2, reward_threshold=225.0\n",
      "176: loss=0.714, reward_mean=150.5, reward_threshold=246.1\n",
      "177: loss=0.730, reward_mean=170.4, reward_threshold=233.9\n",
      "178: loss=0.715, reward_mean=156.1, reward_threshold=247.9\n",
      "179: loss=0.699, reward_mean=164.8, reward_threshold=241.4\n",
      "180: loss=0.650, reward_mean=157.4, reward_threshold=235.3\n",
      "181: loss=0.667, reward_mean=148.0, reward_threshold=240.4\n",
      "182: loss=0.651, reward_mean=120.6, reward_threshold=224.3\n",
      "183: loss=0.693, reward_mean=132.7, reward_threshold=226.0\n",
      "184: loss=0.647, reward_mean=134.6, reward_threshold=238.8\n",
      "185: loss=0.631, reward_mean=125.4, reward_threshold=233.0\n",
      "186: loss=0.610, reward_mean=144.4, reward_threshold=233.4\n",
      "187: loss=0.625, reward_mean=128.1, reward_threshold=232.8\n",
      "188: loss=0.532, reward_mean=136.1, reward_threshold=231.3\n",
      "189: loss=0.551, reward_mean=167.2, reward_threshold=246.0\n",
      "190: loss=0.530, reward_mean=151.1, reward_threshold=241.3\n",
      "191: loss=0.596, reward_mean=160.5, reward_threshold=244.9\n",
      "192: loss=0.523, reward_mean=142.4, reward_threshold=238.3\n",
      "193: loss=0.545, reward_mean=138.6, reward_threshold=250.1\n",
      "194: loss=0.528, reward_mean=151.1, reward_threshold=248.6\n",
      "195: loss=0.578, reward_mean=150.8, reward_threshold=238.1\n",
      "196: loss=0.513, reward_mean=158.9, reward_threshold=239.8\n",
      "197: loss=0.546, reward_mean=152.4, reward_threshold=249.3\n",
      "198: loss=0.557, reward_mean=155.3, reward_threshold=248.8\n",
      "199: loss=0.539, reward_mean=148.4, reward_threshold=238.2\n",
      "200: loss=0.567, reward_mean=125.6, reward_threshold=236.9\n",
      "201: loss=0.553, reward_mean=146.3, reward_threshold=248.8\n",
      "202: loss=0.586, reward_mean=118.9, reward_threshold=231.9\n",
      "203: loss=0.494, reward_mean=156.9, reward_threshold=245.0\n",
      "204: loss=0.446, reward_mean=120.9, reward_threshold=225.6\n",
      "205: loss=0.548, reward_mean=139.9, reward_threshold=235.0\n",
      "206: loss=0.549, reward_mean=156.5, reward_threshold=237.1\n",
      "207: loss=0.562, reward_mean=101.1, reward_threshold=215.3\n",
      "208: loss=0.547, reward_mean=103.8, reward_threshold=229.0\n",
      "209: loss=0.558, reward_mean=106.0, reward_threshold=224.9\n",
      "210: loss=0.619, reward_mean=128.4, reward_threshold=226.0\n",
      "211: loss=0.658, reward_mean=121.6, reward_threshold=225.5\n",
      "212: loss=0.650, reward_mean=151.1, reward_threshold=228.6\n",
      "213: loss=0.667, reward_mean=152.8, reward_threshold=236.9\n",
      "214: loss=0.711, reward_mean=148.9, reward_threshold=240.3\n",
      "215: loss=0.706, reward_mean=130.3, reward_threshold=222.8\n",
      "216: loss=0.743, reward_mean=148.4, reward_threshold=241.4\n",
      "217: loss=0.691, reward_mean=149.9, reward_threshold=231.5\n",
      "218: loss=0.726, reward_mean=155.1, reward_threshold=241.2\n",
      "219: loss=0.763, reward_mean=130.4, reward_threshold=223.9\n",
      "220: loss=0.726, reward_mean=150.1, reward_threshold=242.0\n",
      "221: loss=0.737, reward_mean=162.7, reward_threshold=243.4\n",
      "222: loss=0.677, reward_mean=152.1, reward_threshold=236.9\n",
      "223: loss=0.696, reward_mean=167.4, reward_threshold=250.5\n",
      "224: loss=0.662, reward_mean=149.9, reward_threshold=249.0\n",
      "225: loss=0.634, reward_mean=167.3, reward_threshold=247.7\n",
      "226: loss=0.682, reward_mean=166.5, reward_threshold=248.8\n",
      "227: loss=0.691, reward_mean=161.7, reward_threshold=259.2\n",
      "228: loss=0.683, reward_mean=171.1, reward_threshold=254.8\n",
      "229: loss=0.696, reward_mean=167.6, reward_threshold=249.7\n",
      "230: loss=0.670, reward_mean=171.9, reward_threshold=260.0\n",
      "231: loss=0.709, reward_mean=166.4, reward_threshold=251.1\n",
      "232: loss=0.677, reward_mean=152.8, reward_threshold=245.6\n",
      "233: loss=0.753, reward_mean=158.9, reward_threshold=247.4\n",
      "234: loss=0.722, reward_mean=160.7, reward_threshold=259.8\n",
      "235: loss=0.753, reward_mean=165.8, reward_threshold=256.1\n",
      "236: loss=0.803, reward_mean=145.0, reward_threshold=237.9\n",
      "237: loss=0.808, reward_mean=131.9, reward_threshold=158.7\n",
      "238: loss=0.849, reward_mean=147.7, reward_threshold=192.6\n",
      "239: loss=0.857, reward_mean=147.9, reward_threshold=219.3\n",
      "240: loss=0.853, reward_mean=142.1, reward_threshold=218.2\n",
      "241: loss=0.866, reward_mean=134.9, reward_threshold=163.6\n",
      "242: loss=0.854, reward_mean=138.1, reward_threshold=165.3\n",
      "243: loss=0.828, reward_mean=117.8, reward_threshold=162.4\n",
      "244: loss=0.849, reward_mean=123.4, reward_threshold=166.4\n",
      "245: loss=0.782, reward_mean=126.7, reward_threshold=155.8\n",
      "246: loss=0.817, reward_mean=112.9, reward_threshold=164.8\n",
      "247: loss=0.752, reward_mean=125.3, reward_threshold=182.6\n",
      "248: loss=0.831, reward_mean=109.3, reward_threshold=169.4\n",
      "249: loss=0.805, reward_mean=111.5, reward_threshold=155.6\n",
      "250: loss=0.816, reward_mean=106.9, reward_threshold=159.8\n",
      "251: loss=0.809, reward_mean=119.3, reward_threshold=184.4\n",
      "252: loss=0.786, reward_mean=116.3, reward_threshold=197.0\n",
      "253: loss=0.768, reward_mean=143.9, reward_threshold=225.7\n",
      "254: loss=0.713, reward_mean=157.3, reward_threshold=237.9\n",
      "255: loss=0.728, reward_mean=163.6, reward_threshold=236.1\n",
      "256: loss=0.729, reward_mean=164.8, reward_threshold=247.6\n",
      "257: loss=0.712, reward_mean=162.0, reward_threshold=246.2\n",
      "258: loss=0.746, reward_mean=149.5, reward_threshold=240.4\n",
      "259: loss=0.661, reward_mean=166.2, reward_threshold=248.1\n",
      "260: loss=0.705, reward_mean=166.2, reward_threshold=239.3\n",
      "261: loss=0.688, reward_mean=157.5, reward_threshold=241.8\n",
      "262: loss=0.730, reward_mean=129.4, reward_threshold=227.1\n",
      "263: loss=0.681, reward_mean=160.5, reward_threshold=241.5\n",
      "264: loss=0.769, reward_mean=148.1, reward_threshold=238.5\n",
      "265: loss=0.747, reward_mean=146.1, reward_threshold=236.8\n",
      "266: loss=0.726, reward_mean=141.8, reward_threshold=232.7\n",
      "267: loss=0.680, reward_mean=134.2, reward_threshold=233.8\n",
      "268: loss=0.698, reward_mean=140.9, reward_threshold=236.8\n",
      "269: loss=0.739, reward_mean=136.1, reward_threshold=239.2\n",
      "270: loss=0.752, reward_mean=137.1, reward_threshold=228.0\n",
      "271: loss=0.696, reward_mean=151.6, reward_threshold=237.3\n",
      "272: loss=0.701, reward_mean=130.1, reward_threshold=234.8\n",
      "273: loss=0.746, reward_mean=111.8, reward_threshold=222.1\n",
      "274: loss=0.739, reward_mean=154.8, reward_threshold=236.1\n",
      "275: loss=0.685, reward_mean=125.5, reward_threshold=225.1\n",
      "276: loss=0.696, reward_mean=114.4, reward_threshold=209.7\n",
      "277: loss=0.744, reward_mean=115.8, reward_threshold=221.3\n",
      "278: loss=0.739, reward_mean=109.2, reward_threshold=224.8\n",
      "279: loss=0.687, reward_mean=122.1, reward_threshold=221.1\n",
      "280: loss=0.712, reward_mean=126.6, reward_threshold=226.6\n",
      "281: loss=0.724, reward_mean=122.9, reward_threshold=229.5\n",
      "282: loss=0.715, reward_mean=126.9, reward_threshold=226.8\n",
      "283: loss=0.696, reward_mean=132.9, reward_threshold=225.4\n",
      "284: loss=0.714, reward_mean=155.6, reward_threshold=230.9\n",
      "285: loss=0.729, reward_mean=122.5, reward_threshold=222.7\n",
      "286: loss=0.774, reward_mean=125.3, reward_threshold=230.4\n",
      "287: loss=0.762, reward_mean=149.7, reward_threshold=240.4\n",
      "288: loss=0.736, reward_mean=119.2, reward_threshold=222.0\n",
      "289: loss=0.750, reward_mean=142.8, reward_threshold=234.2\n",
      "290: loss=0.778, reward_mean=117.5, reward_threshold=215.5\n",
      "291: loss=0.725, reward_mean=115.5, reward_threshold=217.6\n",
      "292: loss=0.675, reward_mean=107.6, reward_threshold=228.1\n",
      "293: loss=0.742, reward_mean=115.2, reward_threshold=228.6\n",
      "294: loss=0.701, reward_mean=128.1, reward_threshold=230.0\n",
      "295: loss=0.694, reward_mean=115.3, reward_threshold=218.0\n",
      "296: loss=0.676, reward_mean=120.9, reward_threshold=226.7\n",
      "297: loss=0.661, reward_mean=112.8, reward_threshold=222.6\n",
      "298: loss=0.755, reward_mean=104.8, reward_threshold=224.3\n",
      "299: loss=0.712, reward_mean=125.2, reward_threshold=233.7\n",
      "300: loss=0.665, reward_mean=144.8, reward_threshold=235.0\n",
      "301: loss=0.689, reward_mean=116.9, reward_threshold=228.5\n",
      "302: loss=0.693, reward_mean=141.7, reward_threshold=236.1\n",
      "303: loss=0.712, reward_mean=133.3, reward_threshold=222.5\n",
      "304: loss=0.638, reward_mean=149.3, reward_threshold=224.2\n",
      "305: loss=0.717, reward_mean=123.3, reward_threshold=223.9\n",
      "306: loss=0.738, reward_mean=129.0, reward_threshold=221.7\n",
      "307: loss=0.706, reward_mean=127.4, reward_threshold=222.4\n",
      "308: loss=0.729, reward_mean=137.7, reward_threshold=211.0\n",
      "309: loss=0.731, reward_mean=140.3, reward_threshold=227.6\n",
      "310: loss=0.744, reward_mean=137.9, reward_threshold=214.3\n",
      "311: loss=0.730, reward_mean=150.2, reward_threshold=216.4\n",
      "312: loss=0.736, reward_mean=116.5, reward_threshold=204.8\n",
      "313: loss=0.753, reward_mean=127.6, reward_threshold=200.3\n",
      "314: loss=0.735, reward_mean=134.6, reward_threshold=202.3\n",
      "315: loss=0.707, reward_mean=124.2, reward_threshold=206.5\n",
      "316: loss=0.740, reward_mean=108.6, reward_threshold=184.3\n",
      "317: loss=0.757, reward_mean=104.6, reward_threshold=184.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-97d080417dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     batch_states,batch_actions,batch_rewards = gather_experience_on_environment(\n\u001b[0m\u001b[1;32m      6\u001b[0m         env, policy_net, batch_size, t_max=5000)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ecdde0a79a9f>\u001b[0m in \u001b[0;36mgather_experience_on_environment\u001b[0;34m(env, policy_net, batch_size, t_max)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Act on the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mnew_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Save experience (State,Action,Reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m\"TimeLimit.truncated\"\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \"\"\"\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpassive_env_step_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    527\u001b[0m             )\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_mean_reward = 0\n",
    "\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = gather_experience_on_environment(\n",
    "        env, policy_net, batch_size, t_max=5000)\n",
    "\n",
    "    # Filter a batch of good experiences\n",
    "    elite_states, elite_actions = filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    # Optimise the network a bit\n",
    "    optimizer.zero_grad()\n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "    action_scores_v = policy_net(tensor_states)\n",
    "    loss_v = ce_loss(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_threshold=%.1f\" % (\n",
    "            i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    if mean_reward > best_mean_reward:\n",
    "        print('Save best reward:', mean_reward)\n",
    "        save_best_model(policy_net)\n",
    "        best_mean_reward = mean_reward\n",
    "    \n",
    "    # If the mean reward is bigger than the completion score threshold we stop\n",
    "    if mean_reward > completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model\n",
    "Observe that we never fix the model (.eval()) we really want to always keep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = torch.load('cem_policy_best.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out the best model\n",
    "You can locally run on the enviornment but this command will generate a video, which is better to run from Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordVideo(gym.make(\"LunarLander-v2\"), 'video')\n",
    "gather_experience_on_environment(env, policy_net, 1, t_max=5000)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
