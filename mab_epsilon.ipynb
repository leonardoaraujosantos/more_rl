{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Armed Bandits Epsilon Greedy\n",
    "\n",
    "Here the idea is not to actually to find the leverage with bigger probability of getting immediate money (ie: CTR).\n",
    "But the sequence of actions (arm levers) that will maximise the expected future reward.\n",
    "\n",
    "For example here is not about only chosing the third strategy but the sequence of strategies that gives better reward. Also notice that in real-life you won't have the distributions you want to find (and act) on them.\n",
    "![alt text](imgs/multi_armed_bandit_testbed.png \"Testbed\")\n",
    "\n",
    "On this notebook we will show the simplest MAB method the epsilon greedy, but there are several:\n",
    "* Epsilon Greedy\n",
    "* Thompson Sampling\n",
    "* Upper Confidence Bound 1 (UCB-1)\n",
    "\n",
    "### Regret\n",
    "[Regret](https://vwo.com/blog/multi-armed-bandit-algorithm/) is the \"number\" that represent the following issue:\n",
    "1. You run a statistical test to undertand your customer behaviour\n",
    "2. After weeks of test you start to act\n",
    "3. If you know in advance the result of step 1 and act from the start your profit would be higher\n",
    "\n",
    "Sometimes the issue of regret is even worst when the customer behaviour change while you are testing.\n",
    "\n",
    "### A/B testing: How does it work?\n",
    "In a standard A/B test experiment, we want to measure the likelihood that one variant of a campaign is truly more effective than another, while controlling the probability that our measurement is mistaken:\n",
    "* either that we think there is a winner when there isnâ€™t\n",
    "* or that we miss detecting the winning variant. \n",
    "\n",
    "In order to make accurate measurements, A/B tests must account for two key values:\n",
    "* statistical power: Probability that the experiment will actually detect an effect when an effect exists \n",
    "* statistical significance: Degree of confidence that the effect we measure is actually present.\n",
    "\n",
    "A/B testing is a purely exploratory approach. Since contacts are randomly assigned to each variant with equal probability in an A/B test experiment, the overall payoff achievable equals the average payoff of all the variants, and must be lower than that of the winning variant. As illustrated by the example above, a typical A/B test tends to require hundreds of thousands of contacts to reach appropriate statistical power.\n",
    "\n",
    "### Ok So why MAB\n",
    "Multi-armed bandit (MAB) algorithms can be thought of as alternatives to A/B testing that balance exploitation and exploration during the learning process. A MAB solution uses existing results from the experiment to allocate more contacts to variants that are performing well, while allocating less traffic to variants that are underperforming. The idea here is the following:\n",
    "* You will get the insight you want from the A/B test\n",
    "* While acting optimally when the algorithm start to converge so the regret will always be lower\n",
    "\n",
    "### References\n",
    "* [Introduction](https://markelsanz14.medium.com/introduction-to-reinforcement-learning-part-1-multi-armed-bandit-problem-618e8cbf9d4b)\n",
    "* [Good Article](https://changyaochen.github.io/multi-armed-bandit-mar-2020/)\n",
    "* [Multi Armed Bandits 101](https://medium.com/opex-analytics/multi-armed-bandits-101-6f4ac62b6bd6)\n",
    "* [Other methods](https://towardsdatascience.com/multi-armed-bandits-and-reinforcement-learning-dc9001dcb8da)\n",
    "* [Thompson Sampling](https://towardsdatascience.com/multi-armed-bandits-thompson-sampling-algorithm-fea205cf31df)\n",
    "* [Upper Confidence Bound](https://towardsdatascience.com/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d)\n",
    "* [Comparing Multi-Armed Bandit Algorithms on Marketing Use Cases](https://towardsdatascience.com/comparing-multi-armed-bandit-algorithms-on-marketing-use-cases-8de62a851831)\n",
    "* [Solving the Multi-Armed Bandit Problem from Scratch in Python](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pull_bandit_arm(bandits, bandit_number):\n",
    "    \"\"\"\n",
    "    Even pulling the arm might be random\n",
    "    \"\"\"\n",
    "    result = np.random.uniform()\n",
    "    result_binary = result <= bandits[bandit_number]\n",
    "    return int(result_binary)\n",
    "\n",
    "def take_epsilon_greedy_action(epsilon, average_rewards_actions):\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        # Explore\n",
    "        return np.random.randint(0, len(average_rewards_actions)) \n",
    "    else:\n",
    "        # Greedy action.\n",
    "        return np.argmax(average_rewards_actions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bit of Hope\n",
    "Let's pretend we keep track of the CTR of each of our products in a online system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTR of each strategy\n",
    "market_strategies_ctr = [0.1, 0.3, 0.05, 0.55, 0.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward at step: 0 is ['0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "Expected reward at step: 100 is ['0.06', '0.00', '0.25', '0.51', '0.20']\n",
      "Expected reward at step: 200 is ['0.05', '0.42', '0.17', '0.51', '0.17']\n",
      "Expected reward at step: 300 is ['0.05', '0.43', '0.11', '0.50', '0.12']\n",
      "Expected reward at step: 400 is ['0.05', '0.40', '0.09', '0.52', '0.12']\n",
      "Expected reward at step: 500 is ['0.04', '0.40', '0.14', '0.50', '0.10']\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 500\n",
    "# Actually default value on industry\n",
    "epsilon = 0.1\n",
    "\n",
    "# Store info to know which one is the best action in each moment (Kind of value function)\n",
    "total_rewards = [0 for _ in range(len(market_strategies_ctr))]\n",
    "total_attempts = [0 for _ in range(len(market_strategies_ctr))]\n",
    "avg_rewards = [0.0 for _ in range(len(market_strategies_ctr))]\n",
    "\n",
    "for iteration in range(num_iterations+1):\n",
    "    action = take_epsilon_greedy_action(epsilon, avg_rewards)\n",
    "    reward = pull_bandit_arm(market_strategies_ctr, action)\n",
    "    # Store result.\n",
    "    total_rewards[action] += reward\n",
    "    total_attempts[action] += 1\n",
    "    avg_rewards[action] = total_rewards[action] / float(total_attempts[action])\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print('Expected reward at step: {} is {}'.format(\n",
    "            iteration,['{:.2f}'.format(elem) for elem in avg_rewards]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Best Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best bandit is 3 with an mean reward of 0.498\n",
      "Total mean reward in the 500 episodes has been 228\n"
     ]
    }
   ],
   "source": [
    "best_bandit = np.argmax(avg_rewards)\n",
    "print('\\nBest bandit is {} with an mean reward of {:.3f}'.format(\n",
    "    best_bandit, avg_rewards[best_bandit]))\n",
    "print('Total mean reward in the {} episodes has been {}'\n",
    "      .format(num_iterations, sum(total_rewards)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
