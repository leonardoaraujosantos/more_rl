{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Armed Bandits Epsilon Greedy\n",
    "\n",
    "### References\n",
    "* [Introduction](https://markelsanz14.medium.com/introduction-to-reinforcement-learning-part-1-multi-armed-bandit-problem-618e8cbf9d4b)\n",
    "* [Good Article](https://changyaochen.github.io/multi-armed-bandit-mar-2020/)\n",
    "* [Multi Armed Bandits 101](https://medium.com/opex-analytics/multi-armed-bandits-101-6f4ac62b6bd6)\n",
    "* [Other methods](https://towardsdatascience.com/multi-armed-bandits-and-reinforcement-learning-dc9001dcb8da)\n",
    "* [Thompson Sampling](https://towardsdatascience.com/multi-armed-bandits-thompson-sampling-algorithm-fea205cf31df)\n",
    "* [Upper Confidence Bound](https://towardsdatascience.com/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pull_bandit_arm(bandits, bandit_number):\n",
    "    \"\"\"\n",
    "    Even pulling the arm might be random\n",
    "    \"\"\"\n",
    "    result = np.random.uniform()\n",
    "    result <= bandits[bandit_number]\n",
    "    return int(result)\n",
    "\n",
    "def take_epsilon_greedy_action(epsilon, average_rewards_actions):\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        # Explore\n",
    "        return np.random.randint(0, len(average_rewards_actions)) \n",
    "    else:\n",
    "        # Greedy action.\n",
    "        return np.argmax(average_rewards_actions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bit of Hope\n",
    "Let's pretend we keep track of the CTR of each of our products in a online system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTR of each strategy\n",
    "market_strategies_ctr = [0.1, 0.3, 0.05, 0.55, 0.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward at step: 0 is ['0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "Expected reward at step: 100 is ['0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "Expected reward at step: 200 is ['0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "Expected reward at step: 300 is ['0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "Expected reward at step: 400 is ['0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "Expected reward at step: 500 is ['0.00', '0.00', '0.00', '0.00', '0.00']\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 500\n",
    "epsilon = 0.1\n",
    "\n",
    "# Store info to know which one is the best action in each moment.\n",
    "total_rewards = [0 for _ in range(len(market_strategies_ctr))]\n",
    "total_attempts = [0 for _ in range(len(market_strategies_ctr))]\n",
    "avg_rewards = [0.0 for _ in range(len(market_strategies_ctr))]\n",
    "\n",
    "for iteration in range(num_iterations+1):\n",
    "    action = take_epsilon_greedy_action(epsilon, avg_rewards)\n",
    "    reward = pull_bandit_arm(market_strategies_ctr, action)\n",
    "    # Store result.\n",
    "    total_rewards[action] += reward\n",
    "    total_attempts[action] += 1\n",
    "    avg_rewards[action] = total_rewards[action] / float(total_attempts[action])\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print('Expected reward at step: {} is {}'.format(\n",
    "            iteration,['{:.2f}'.format(elem) for elem in avg_rewards]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Best Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best bandit is 0 with an mean reward of 0.000\n",
      "Total mean reward in the 500 episodes has been 0\n"
     ]
    }
   ],
   "source": [
    "best_bandit = np.argmax(avg_rewards)\n",
    "print('\\nBest bandit is {} with an mean reward of {:.3f}'.format(\n",
    "    best_bandit, avg_rewards[best_bandit]))\n",
    "print('Total mean reward in the {} episodes has been {}'\n",
    "      .format(num_iterations, sum(total_rewards)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
