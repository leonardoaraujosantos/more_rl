{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming (Policy Iteration)\n",
    "\n",
    "### Policy/Value improvement\n",
    "During the algorithm execution both value and policy improves at the same time.\n",
    "![alt text](imgs/policy_value_improvement.png \"Game\")\n",
    "\n",
    "### Policy Iteration vs Value Iteration\n",
    "* Both algorithms converge\n",
    "* Policy iteration converge with less iterations but is more computation intensieve\n",
    "* Both need a model from the environment (MDP)\n",
    "\n",
    "### Policy vs Value iteration\n",
    "One drawback to policy iteration is that each of its iterations involves policy evaluation, besides the policy improvement so the value iteration is easier to compute, but needs more iterations.\n",
    "\n",
    "### References\n",
    "* https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa\n",
    "* https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
    "* https://en.wikipedia.org/wiki/Bellman_equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyper parameters\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardo_a/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "#env_name  = 'FrozenLake8x8-v0'\n",
    "#env_name = 'FrozenLake-v0'\n",
    "env_name = 'Taxi-v2'\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Environment Model\n",
    "This part of the code will expose a dictionary $P(state,action)$ that will return a list of tupples $(prob,\\text{next_state},reward,done)$, this will be the list of possible next states from that particular (state,action) pair, while P exposes the MDP of the environment.\n",
    "\n",
    "For example this code would return the same thing\n",
    "```python\n",
    "state_reset = env.reset()\n",
    "print('reset state:',state_reset)\n",
    "action=0\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print('############')\n",
    "print('Next state:',next_state)\n",
    "print('Reward:',reward)\n",
    "print('done:',done)\n",
    "print('info:',info)\n",
    "\n",
    "#reset state: 0\n",
    "############\n",
    "#Next state: 0\n",
    "#Reward: 0.0\n",
    "#done: False\n",
    "#info: {'prob': 0.3333333333333333}\n",
    "\n",
    "# Return all the possible states with the tupple (probability, next_state, reward, done)\n",
    "env.P[state_reset][action]\n",
    "# [(0.3333333333333333, 0, 0.0, False),\n",
    "# (0.3333333333333333, 0, 0.0, False),\n",
    "# (0.3333333333333333, 8, 0.0, False)]\n",
    "```\n",
    "\n",
    "Just few environments expose their MDP:\n",
    "* FrozenLake-v0\n",
    "* FrozenLake8x8-v0\n",
    "* Taxi-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have access to the MDP of the environment\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display search space size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space: 3000\n"
     ]
    }
   ],
   "source": [
    "num_states = None\n",
    "num_actions = None\n",
    "try:\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    print('Search space:',num_states*num_actions)\n",
    "except:\n",
    "    num_states = np.prod(env.observation_space.shape)\n",
    "    num_actions = np.prod(env.action_space.shape)\n",
    "    print('Search space:',num_states*num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement\n",
    "Extract a policy given some value function estimation. When the value estimation converges to the real value function we will get the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(v, gamma = 1.0):\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            q[s,a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
    "        \n",
    "        # If we have the action-value function we can just act greedly and thet the action with biggest value\n",
    "        policy[s] = np.argmax(q[s,:])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation\n",
    "Improves the value function under a policy. \n",
    "This function could also be solved as a linear equation instead of an interative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_v_under_policy(env, policy, gamma=1.0):\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "\n",
    "    # Improves the value under a given policy\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            # Get action from given policy\n",
    "            a = policy[s]\n",
    "            \n",
    "            # Bellman Expectation equation\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][a]])\n",
    "        \n",
    "        # Check convergence (Basically check if the value function didn't change much)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= np.finfo(float).eps):\n",
    "            #print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration Algorithm\n",
    "The algorithm has the following structure:\n",
    "1. Start with a random policy.\n",
    "2. Find the value function of that policy.\n",
    "3. Find new policy based on the previous value function\n",
    "4. Go back to 2 until the policy don't change anymore\n",
    "\n",
    "The policy is evaluated using the Bellman Function.\n",
    "![alt text](imgs/policy_iteration.png \"Game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma = 1.0, max_iterations = 200000):\n",
    "    # initialize a random policy\n",
    "    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n))  \n",
    "    \n",
    "    # Iterate a lot...\n",
    "    for i in range(max_iterations):\n",
    "        # Policy Evaluation\n",
    "        old_policy_v = evaluate_v_under_policy(env, policy, gamma)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        new_policy = policy_improvement(old_policy_v, gamma)\n",
    "        \n",
    "        # Detect convergence\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Policy Iteration Algorithm\n",
    "Compare between policy and value iteration the number of iterations and time to compute for the same environment.\n",
    "\n",
    "#### Result against Value Iteration on Taxi driver problem\n",
    "* Value iteration took 3270 iterations\n",
    "* 14 seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 16.\n",
      "CPU times: user 37.9 s, sys: 321 ms, total: 38.2 s\n",
      "Wall time: 38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimal_policy = policy_iteration(env, gamma = gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 12\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "while not done:\n",
    "    action = int(optimal_policy[state])\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    state = next_state\n",
    "    step += 1\n",
    "    print(\"Step: {}\".format(step))\n",
    "    env.render()\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
